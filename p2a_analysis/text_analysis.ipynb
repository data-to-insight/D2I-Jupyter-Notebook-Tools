{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Used to analyse p2a megamatrix\n",
    "'''\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.read_excel(r'/workspaces/D2I-Jupyter-Notebook-Tools/p2a_analysis/mega matrix - with analysis.xlsx',\n",
    "                  'All data items')\n",
    "df.columns = df.columns.str.lower()\n",
    "questions_df = df[['croydon text', 'essex text', 'sutton text', 'camden text']]\n",
    "df['croydon text'] = df['croydon text'].astype(str).str.lower()\n",
    "df['essex text'] = df['essex text'].astype(str).str.lower()\n",
    "df['sutton text'] = df['sutton text'].astype(str).str.lower()\n",
    "df['camden text'] = df['camden text'].astype(str).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Turning all words into regular expressions\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "df['croydon token'] = df['croydon text'].apply(regexp.tokenize)\n",
    "df['sutton token'] = df['sutton text'].apply(regexp.tokenize)\n",
    "df['essex token'] = df['essex text'].apply(regexp.tokenize)\n",
    "df['camden token'] = df['camden text'].apply(regexp.tokenize)\n",
    "\n",
    "# Adding stopwords and removing from data\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "my_stopwords = ['child', 'young', 'person']\n",
    "stopwords.extend(my_stopwords)\n",
    "df['camden token'] = df['camden token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df['sutton token'] = df['sutton token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df['essex token'] = df['essex token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "df['croydon token'] = df['croydon token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "\n",
    "# Keep only words longer than 2 letter\n",
    "df['essex string'] = df['essex token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "df['camden string'] = df['camden token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "df['croydon string'] = df['croydon token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "df['sutton string'] = df['sutton token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "\n",
    "#  Lists of all words\n",
    "sutton_words = ' '.join([word for word in df['sutton string']])\n",
    "camden_words = ' '.join([word for word in df['camden string']])\n",
    "essex_words = ' '.join([word for word in df['essex string']])\n",
    "croydon_words = ' '.join([word for word in df['croydon string']])\n",
    "\n",
    "# Tokenize all and frequency distribution\n",
    "sutton_tokenized = nltk.tokenize.word_tokenize(sutton_words)\n",
    "camden_tokenized = nltk.tokenize.word_tokenize(camden_words)\n",
    "essex_tokenized = nltk.tokenize.word_tokenize(essex_words)\n",
    "croydon_tokenized = nltk.tokenize.word_tokenize(croydon_words)\n",
    "#Drop words appearing less than 3 times\n",
    "sutton_fdist = FreqDist(sutton_tokenized)\n",
    "essex_fdist = FreqDist(essex_tokenized)\n",
    "camden_fdist = FreqDist(camden_tokenized)\n",
    "croydon_fdist = FreqDist(croydon_tokenized)\n",
    "df['sutton string fdist'] = df['sutton token'].apply(lambda x: ' '.join([item for item in x if sutton_fdist[item] >= 3 ]))\n",
    "df['essex string fdist'] = df['essex token'].apply(lambda x: ' '.join([item for item in x if essex_fdist[item] >= 3 ]))\n",
    "df['camden string fdist'] = df['camden token'].apply(lambda x: ' '.join([item for item in x if camden_fdist[item] >= 3 ]))\n",
    "df['croydon string fdist'] = df['croydon token'].apply(lambda x: ' '.join([item for item in x if croydon_fdist[item] >= 3 ]))\n",
    "\n",
    "# Lemmatization - grouping together similar words\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "\n",
    "df['sutton_lem'] = df['sutton string fdist'].apply(wordnet_lem.lemmatize)\n",
    "df['essex lem'] = df['essex string fdist'].apply(wordnet_lem.lemmatize)\n",
    "df['camden lem'] = df['camden string fdist'].apply(wordnet_lem.lemmatize)\n",
    "df['croydon lem'] = df['croydon string fdist'].apply(wordnet_lem.lemmatize)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

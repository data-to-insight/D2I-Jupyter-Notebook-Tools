{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ofsted Inspection Reports Scrape Tool \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary:</b><br>\n",
    "<span style=\"font-size:10pt\">Scrapes all local authorities 'Childrens Services Ofsted Inspection' reports* creating (an enhanced version) the 'Ofsted ILACS Outcomes summary' report**. Output is optionally in either csv or direct to xls format. As well as creating the summary report, the process scrapes/downloads and organises into named folders*** the Children's Services <b>full</b> inspection reports(i.e. not interim, focused nor monitoring visits) in their original pdf. Each LA name is 'cleaned' to aid a standard/onward process use of LA naming - e.g. 'Nottingham City Council' becomes 'Nottingham' and 'Barnsley Metropolitan Borough Council' becomes 'Barnsley'. </span><br>\n",
    "    \n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>**Available at: https://reports.ofsted.gov.uk/ .</li>\n",
    "    <li>*Available at: https://adcs.org.uk/inspection/article/ilacs-outcomes-summary </li>\n",
    "    <li>***folder structure : \\\\export_data\\inspection_reports\\provider_urn+local_authority_name(lowercase)...pdf.</li>\n",
    "</ul> \n",
    "\n",
    "<br>\n",
    "<b>Exports:</b><br>\n",
    "<span style=\"font-size:10pt\">Static data can be added to further enrich the current summary output. As an example the Local Authority Number is added within this process. This process has been structured in a manner that would easily provide the mechanisms to enable further data enrichment, e.g. geospacial providing a suitable key column can be accessed. </span><br>\n",
    "\n",
    "`script root`\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>\\export_data\\\n",
    "        <ul>\n",
    "            <li>\\inspection_reports\\</li>\n",
    "                <ul>\n",
    "                <li>\\provider_urn+local_authority_name\\*.pdf</li>\n",
    "                </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<b>Imports:</b><br>\n",
    "<span style=\"font-size:10pt\">Static data can be added to further enrich the current summary output. As an example the Local Authority Number is added within this process. This process has been structured in a manner that would easily provide the mechanisms to enable further data enrichment, e.g. geospacial providing a suitable key column can be accessed. </span><br>\n",
    "\n",
    "`script root`\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>\\import_data\\\n",
    "        <ul>\n",
    "            <li>\\la_lookup\\</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<b>N.B/Pre-requisites:</b><br>\n",
    "<span style=\"font-size:10pt\">Relies on Ofsted's continued use of nonvisual css element descriptors on the web site. Obv not ideal to rely on anything in the web-space, but any scrape process, however robust, is undermined/dictated by subsequent page changes. The tool has avoided the use of Selenium or similar as this is more likely to be impacted by visual design changes on the page(s). Instead it relies on the underlying php search process, and associated php generated links.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backlog/to-do:</b><br>\n",
    "\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>Moved to Trello: https://trello.com/c/4TihKpvQ</li>\n",
    "\n",
    "</ul> \n",
    "\n",
    "<b>Known bugs:</b><br>\n",
    "\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>Moved to Trello</li>\n",
    "    \n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export options\n",
    "\n",
    "export_summary_filename = 'ofsted_childrens_services_overview'\n",
    "# export_file_type         = 'csv' # Excel / csv currently supported\n",
    "export_file_type         = 'excel' \n",
    "\n",
    "# Default (sub)folder structure\n",
    "# Defined to offer some ease of onward flexibility\n",
    "root_export_folder = 'export_data'              # <all> exports folder\n",
    "inspections_subfolder = 'inspection_reports'    # downloaded report pdfs\n",
    "\n",
    "\n",
    "# scrape inspection grade/data from pdf reports\n",
    "pdf_data_capture = True # True is default (scrape within pdf inspection reports for inspection results etc)\n",
    "                        # This impacts run time E.g False == ~1m20 / True == ~ 4m10\n",
    "                        # False == only pdfs/list of LA's+link to most recent exported. Not inspection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ofsted site/page admin settings\n",
    "\n",
    "short_inspection_threshold    = 7 # ILACS inspection duration in days\n",
    "standard_inspection_threshold = 14\n",
    "\n",
    "max_page_results = 200 # Set max number of search results to show on page(MUST be > total number of LA's!) \n",
    "url_stem = 'https://reports.ofsted.gov.uk/'\n",
    "search_url = 'search?q=&location=&lat=&lon=&radius=&level_1_types=3&level_2_types%5B%5D=12' # On to-do list\n",
    "max_page_results_url = '&rows=' + str(max_page_results) # Coerce results page to display ALL providers on single results page without next/pagination\n",
    "\n",
    "# resultant complete url to process\n",
    "url = url_stem + search_url + max_page_results_url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Script admin settings\n",
    "\n",
    "# Keep warnings quiet unless priority\n",
    "import logging\n",
    "import subprocess\n",
    "import warnings\n",
    "logging.getLogger('org.apache.pdfbox').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non-standard modules that might need installing\n",
    "# !pip install PyPDF2\n",
    "# !pip install tabula-py\n",
    "# !pip install textblob\n",
    "# !pip install gensim\n",
    "# !pip install matplotlib\n",
    "# !pip install openpyxl\n",
    "# !pip install XlsxWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# pdf search/data extraction\n",
    "import io\n",
    "import os\n",
    "import tabula   \n",
    "import PyPDF2   \n",
    "import re       \n",
    "\n",
    "# used in handling inspection dates\n",
    "from dateutil import parser \n",
    "from datetime import datetime\n",
    "\n",
    "# nlp stuff for sentiment\n",
    "from textblob import TextBlob\n",
    "from gensim import corpora, models\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# handle optional excel export+active file links\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import xlsxwriter\n",
    "\n",
    "\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function defs\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given a URL, returns a BeautifulSoup object.\n",
    "    Args: url (str): The URL to fetch and parse.\n",
    "    Returns: BeautifulSoup: The parsed HTML content.\n",
    "    \"\"\"\n",
    "    timeout_seconds = 10 # lets not assume the Ofsted page is up\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout_seconds)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_provider_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the la/provider name according to:\n",
    "                - expected output based on existing ILACS sheet\n",
    "                - historic string issues seen on Ofsted site\n",
    "\n",
    "    Args:\n",
    "        name (str): The original name to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned name.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    name = name.lower().replace('  ', ' ')\n",
    "    \n",
    "    # Remove specific phrases\n",
    "    name = name.replace(\"royal borough of \", \"\").replace(\"city of \", \"\").replace(\"metropolitan district council\", \"\").replace(\"london borough of\", \"\").replace(\"council of\", \"\")\n",
    "    \n",
    "    # Remove further undesired 'single' words and join the remaining parts\n",
    "    name_parts = [part for part in name.split() if part not in ['city', 'metropolitan', 'borough', 'council', 'county', 'district', 'the']]\n",
    "    return ' '.join(name_parts)\n",
    "\n",
    "\n",
    "def get_framework_type(start_date, end_date, short_inspection_threshold, standard_inspection_threshold):\n",
    "    \"\"\"\n",
    "    Returns an inspection framework type based on the duration between the start and end dates.\n",
    "    Dates are scraped, as this currently the only ref. This not ideal as based entirely\n",
    "    on varied formats of text based data. Therefore some cleaning included here. \n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in the format \"dd/mm/yyyy\".\n",
    "        end_date (str): End date in the format \"dd/mm/yyyy\".\n",
    "\n",
    "    Returns:\n",
    "        str: Inspection framework type, which can be \"short\", \"standard\", or \"inspection duration longer than standard framework\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if both start and end dates have been accessible\n",
    "    if start_date is not None and end_date is not None:\n",
    "\n",
    "        # Check if end date is not earlier than start date\n",
    "        if end_date < start_date:\n",
    "            inspection_framework_str = \"invalid end or start date extracted\"\n",
    "\n",
    "        # Calculate the number of days between inspection start and end dates\n",
    "        else:\n",
    "            delta = end_date - start_date\n",
    "            inspection_duration_days = delta.days\n",
    "\n",
    "            # Determine the inspection framework based on the duration days\n",
    "            # Note: Needs further investigation to sense check real-world timeframes here, i.e. are thresholds 'working days'?\n",
    "            # For most instances this appears to be sufficiently accurate as-is. \n",
    "            if inspection_duration_days <= short_inspection_threshold:\n",
    "                inspection_framework_str = \"short\"\n",
    "            elif short_inspection_threshold < inspection_duration_days <= standard_inspection_threshold + 1:\n",
    "                inspection_framework_str = \"standard\"\n",
    "            else:\n",
    "                inspection_framework_str = \"inspection duration longer than standard framework\"\n",
    "\n",
    "    # Handle cases where start or end date is not provided \n",
    "    # Note: end date most likely to have not been extracted due to formatting issues\n",
    "    else:\n",
    "        inspection_framework_str = \"invalid date format\"\n",
    "\n",
    "    return inspection_framework_str\n",
    "\n",
    "\n",
    "def format_date(date_str: str, input_format: str, output_format: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert and format a date string.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): The input date string.\n",
    "        input_format (str): The format of the input date string.\n",
    "        output_format (str): The desired output format.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_str, input_format)\n",
    "    date_obj = dt.date()\n",
    "\n",
    "    return date_obj.strftime(output_format)\n",
    "\n",
    "\n",
    "def parse_date(date_str, input_format):\n",
    "    dt = datetime.strptime(date_str, input_format)\n",
    "\n",
    "    return dt.date()\n",
    "\n",
    "\n",
    "def format_date_for_report(date_obj, output_format_str):\n",
    "    \"\"\"\n",
    "    Formats a datetime object as a string in the d/m/y format, or returns an empty string if the input is None.\n",
    "\n",
    "    Args:\n",
    "        date_obj (datetime.datetime or None): The datetime object to format, or None.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted date string, or an empty string if date_obj is None.\n",
    "    \"\"\"\n",
    "    if date_obj is not None:\n",
    "        return date_obj.strftime(output_format_str)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_inspection_grade(row, column_name):\n",
    "    \"\"\"\n",
    "    Extracts the grade from the given row and column name. If the grade contains\n",
    "    the phrase \"requires improvement\", it returns the cleaned-up value.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from a Pandas DataFrame.\n",
    "        column_name (str): The name of the column containing the grade.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted grade.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the grade value cannot be converted to a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grade = str(row[column_name])\n",
    "\n",
    "        if \"requires improvement\" in grade.lower():\n",
    "            # Some RI text has further comment that we don't want, i.e. 'RI, *to become good*' \n",
    "            grade = \"Requires improvement\"\n",
    "        return grade\n",
    "    except Exception as e:\n",
    "        grade = f\"Unknown value type : {grade}\"\n",
    "        error_msg = f\"unknown value found: \\\"unknown : {grade}\\\"\"\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "\n",
    "\n",
    "def extract_inspection_data(pdf_content):\n",
    "    \"\"\"\n",
    "    Extracts the inspector's name, overall Ofsted grade, and inspection dates from the first page of a PDF report.\n",
    "\n",
    "    Args:\n",
    "        pdf_content (bytes): The content of the PDF file as bytes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing results extracted from the ofsted inspection report(s) \n",
    "        Incl. inspector's name, overall Ofsted grade, inspection dates, or None if not found.\n",
    "        Additional extracted data can easily be added here, but must be added to the returned dict. \n",
    "\n",
    "    Notes:\n",
    "        This function extracts information from the first page of the PDF report. The inspector's name is extracted using a\n",
    "        regular expression search for the string \"Lead inspector:\". The overall Ofsted grade is extracted from a table that\n",
    "        appears on the first page of the report. The function uses the tabula library to extract the table data. The inspection\n",
    "        dates are also extracted using a regular expression search for the string \"Inspection dates:\". The function attempts to\n",
    "        parse the inspection dates into datetime objects and format them as \"dd/mm/yyyy\". The final output is a dictionary\n",
    "        containing the extracted information or None if any of the information could not be found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when an unknown grade value is found during grade extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a file-like buffer for the PDF content\n",
    "    with io.BytesIO(pdf_content) as buffer:\n",
    "        # Read the PDF content for text extraction\n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        \n",
    "        # Extract the first page of inspection report pdf\n",
    "        # This to ensure when we iterate/search the summary table, chance of invalid table reduced\n",
    "        first_page_text = reader.pages[0].extract_text()\n",
    "\n",
    "        # Extract text from <all> pages in the pdf\n",
    "        full_text = ''\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text()\n",
    "\n",
    "        # Call the get_sentiment_and_topics function\n",
    "        sentiment_val, key_inspection_themes_lst = get_sentiment_and_topics(buffer)\n",
    "\n",
    "\n",
    "        # Convert val to a <general> sentiment text/str for (readable) reporting\n",
    "        sentiment_summary_str = get_sentiment_category(sentiment_val)\n",
    "\n",
    "\n",
    "        # #################\n",
    "        # # testing / in dev-progress\n",
    "        # print(sentiment_summary_str) # testing\n",
    "        # # Call the updated get_sentiment** function # testing\n",
    "        # sentiment_val2, filtered_themes = get_sentiment_and_sentiment_by_theme(buffer, \"leadership\", \"results\", \"management\") # testing\n",
    "        # plot_filtered_topics(filtered_themes) # testing\n",
    "        # #################\n",
    "\n",
    "\n",
    "\n",
    "        # Find the inspector's name using a regular expression\n",
    "        match = re.search(r\"Lead inspector:\\s*(.+)\", first_page_text)\n",
    "        if match:\n",
    "            inspector_name = match.group(1)\n",
    "            \n",
    "            inspector_name = inspector_name.split(',')[0].strip()       # Remove everything after the first comma (some contain '.., Her Majesty’s Inspector')\n",
    "            inspector_name = inspector_name.replace(\"HMI\", \"\").rstrip() # Remove \"HMI\" and any trailing spaces(some inspectors add this to name)\n",
    "\n",
    "        else:\n",
    "            inspector_name = None\n",
    "\n",
    "        # Read the PDF and extract the table on the first page\n",
    "        try:\n",
    "            buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "            tables = tabula.read_pdf(buffer, pages=1, multiple_tables=True)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the PDF: {e}\")\n",
    "            tables = []\n",
    "\n",
    "    # Initialize variables to store inspection grades\n",
    "    inspection_grade = None\n",
    "    impact_of_leaders_grade_str = None\n",
    "    help_and_protection_grade_str = None\n",
    "    care_and_care_leavers_grade_str = None\n",
    "\n",
    "    # Loop through tables to find the table containing inspection grades\n",
    "    # (Obv at the moment only 1, but just in case someone adds another)\n",
    "\n",
    "    # Loop through tables to find the table containing grades\n",
    "    for table in tables:\n",
    "        # Check if table contains necessary columns\n",
    "        if 'Judgement' in table.columns and 'Grade' in table.columns:\n",
    "            # Iterate through rows of the table\n",
    "            for index, row in table.iterrows():\n",
    "                # Convert judgement to lower case for case-insensitive matching\n",
    "                # Check if the value is NaN or Null and convert judgement to lower case for case-insensitive matching\n",
    "                if pd.isna(row['Judgement']):\n",
    "                    judgement = ''\n",
    "                else:\n",
    "                    judgement = str(row['Judgement']).lower()\n",
    "\n",
    "                # Check if report summary table/row contains 'Overall effectiveness'\n",
    "                if 'overall effectiveness' == judgement:\n",
    "                    inspection_grade = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The impact of leaders on social work practice with children and families'\n",
    "                elif re.search('impact of leaders', judgement):\n",
    "                    impact_of_leaders_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The experiences and progress of children who need help and protection'\n",
    "                elif re.search('need help', judgement):\n",
    "                    help_and_protection_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The experiences and progress of children in care and care leavers'\n",
    "                elif re.search('in care', judgement):\n",
    "                    care_and_care_leavers_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "\n",
    "            # If inspection_grade is found and all other optional grades are found or not required, exit the loop\n",
    "            if inspection_grade is not None:\n",
    "                optional_grades = [impact_of_leaders_grade_str, help_and_protection_grade_str, care_and_care_leavers_grade_str]\n",
    "                if all(grade is not None for grade in optional_grades) or any(grade is None for grade in optional_grades):\n",
    "                    break\n",
    "\n",
    "\n",
    "    # Find the inspection dates using a regular expression\n",
    "    date_match = re.search(r\"Inspection dates:\\s*(.+)\", first_page_text)\n",
    "\n",
    "    if date_match:\n",
    "        # IF there was date data\n",
    "\n",
    "\n",
    "        inspection_dates = date_match.group(1).strip()\n",
    "            \n",
    "        # Some initial clean up based on historic data obs\n",
    "        inspection_dates = inspection_dates.replace(\".\", \"\")\n",
    "        inspection_dates = inspection_dates.replace(\"\\u00A0\", \" \") # Remove non-breaking space (Seen in nottingham report)\n",
    "        inspection_dates = re.sub(r\"[\\u2012\\u2013\\u2014\\u2212\\-]+\", \" to \", inspection_dates) # replace en dash char (\"\\u2013\"), em dash (\"\\u2014\"), or (\"-\") \n",
    "        inspection_dates = inspection_dates.split(\"and\")[0].strip() # Need this because we have such as :\n",
    "                                                                    # \"8 July 2019 to 12 July 2019 and 7 August 2019 to 8 August 2019\"\n",
    "                                                                    # E.g. Derbyshire\n",
    "        inspection_dates = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', inspection_dates) # Fix white spaces between date numbers e.g. \"wiltshire,\t1 9 June 2019\"\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(inspection_dates, str):\n",
    "            # data was as expected\n",
    "            year_match = re.search(r\"\\d{4}\", inspection_dates)\n",
    "            if year_match:\n",
    "                year = year_match.group(0) # get single copy of yyyy\n",
    "\n",
    "                # Now remove the year from the inspection_dates string\n",
    "                inspection_dates_cleaned = inspection_dates.replace(year, \"\").strip()\n",
    "\n",
    "            else:\n",
    "                # We had inspection_dates data but no recognisable year\n",
    "                year = None\n",
    "                inspection_dates_cleaned = inspection_dates\n",
    "\n",
    "        else:\n",
    "            # spurious data\n",
    "            # inspection_dates arrived with non-str, set default val\n",
    "            print(\"Error: inspection_dates is not a string. Type is\", type(inspection_dates))\n",
    "            inspection_dates_cleaned = None \n",
    "\n",
    "\n",
    "        # Now that we have already removed/cleaned those with 'and .....'\n",
    "        # Split the inspection_dates_cleaned string using ' to ' as the delimiter and limit the number of splits to 1\n",
    "        date_parts = inspection_dates_cleaned.split(' to ', maxsplit=1) # expect only 1 instance of 'to' between date vals\n",
    "        \n",
    "\n",
    "  \n",
    "        # Get the seperate inspection date(s) \n",
    "        start_date = date_parts[0].strip()\n",
    "        end_date = date_parts[1].strip() if len(date_parts) > 1 else None\n",
    "        \n",
    "        # Check if the month text is written in *both* the date strings\n",
    "        # Required work-around as Ofsted reports contain inspection date strings in multiple formats (i/ii/iii...)\n",
    "        #   i)      \"15 to 26 November\"  \n",
    "        #   ii)     \"28 February to 4 March\" or \"8 October to 19 October\" (majority)\n",
    "        #   iii)    ['8 July ', '12 July   and 7 August  to'] (*recently seen)\n",
    "        #   iv)     \"11 September 2017 to 5 October 2017\" (double year)\n",
    "        #   v)      \"Inspection dates: 19 November–30 November 2018\" (Bromley)\n",
    "        if len(start_date) <= 2: # i.e. do we only have a date with no month text\n",
    "            inspection_month = end_date.split()[1]\n",
    "            start_date = f\"{start_date} {inspection_month}\"\n",
    "\n",
    "        # Append the inspection year to the start_date and end_date\n",
    "        start_date_str = f\"{start_date} {year}\"\n",
    "        end_date_str = f\"{end_date} {year}\" if end_date else None\n",
    "\n",
    "\n",
    "        # format current str dates (as dt objects)\n",
    "        start_date_formatted = parse_date(start_date_str, '%d %B %Y') #  str '8 January 2021' \n",
    "        end_date_formatted = parse_date(end_date_str, '%d %B %Y')\n",
    "\n",
    "        # calculate inspection duration and return framework string\n",
    "        # Note: Problems arising here generally relate to the end_date extraction from pdf\n",
    "        inspection_framework_str = get_framework_type(start_date_formatted, end_date_formatted, short_inspection_threshold, standard_inspection_threshold)\n",
    "\n",
    "    else:\n",
    "        # unable to extract the data or didnt exist\n",
    "        start_date_formatted = None\n",
    "        end_date_formatted = None\n",
    "        inspection_framework_str = None\n",
    "\n",
    "\n",
    "    return {'inspector_name': inspector_name, \n",
    "            'overall_inspection_grade': inspection_grade,\n",
    "            'inspection_start_date': start_date_formatted,\n",
    "            'inspection_end_date': end_date_formatted,\n",
    "            'inspection_framework': inspection_framework_str,\n",
    "            'impact_of_leaders_grade': impact_of_leaders_grade_str,\n",
    "            'help_and_protection_grade': help_and_protection_grade_str,\n",
    "            'care_and_care_leavers_grade': care_and_care_leavers_grade_str,\n",
    "            'sentiment_score': sentiment_val, \n",
    "            'sentiment_summary': sentiment_summary_str,\n",
    "            'main_inspection_topics': key_inspection_themes_lst\n",
    "            }\n",
    "\n",
    "\n",
    "def process_provider_links(provider_links):\n",
    "    \"\"\"\n",
    "    Processes provider links and returns a list of dictionaries containing URN, local authority, and inspection link.\n",
    "\n",
    "    Args:\n",
    "        provider_links (list): A list of BeautifulSoup Tag objects representing provider links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing URN, local authority, inspection link, and, if enabled, additional inspection data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    global pdf_data_capture # Bool flag\n",
    "    global root_export_folder\n",
    "    global inspections_subfolder\n",
    "\n",
    "\n",
    "    for link in provider_links:\n",
    "        # Extract the URN and provider name from the web link shown\n",
    "        urn = link['href'].rsplit('/', 1)[-1]\n",
    "        name = clean_provider_name(link.text.strip())\n",
    "\n",
    "\n",
    "        clean_provider_dir = os.path.join(root_export_folder, inspections_subfolder, urn + '_' + name)\n",
    "        provider_dir = os.path.join('.', root_export_folder, inspections_subfolder, urn + '_' + name)\n",
    "\n",
    "\n",
    "        # Create the provider directory if it doesn't exist\n",
    "        if not os.path.exists(provider_dir):\n",
    "            os.makedirs(provider_dir)\n",
    "\n",
    "        # Get the child page content\n",
    "        child_url = 'https://reports.ofsted.gov.uk' + link['href']\n",
    "        child_soup = get_soup(child_url)\n",
    "\n",
    "        # Find all publication links in the provider's child page\n",
    "        pdf_links = child_soup.find_all('a', {'class': 'publication-link'})\n",
    "\n",
    "        # Initialize a flag to indicate if an inspection link has been found\n",
    "        # Important: This assumes that the provider's reports are returned/organised most recent FIRST\n",
    "        found_inspection_link = False\n",
    "\n",
    "        # Iterate through the publication links\n",
    "        for pdf_link in pdf_links:\n",
    "\n",
    "            # Check if the current/next href-link meets the selection criteria\n",
    "            # This block obv relies on Ofsted continued use of nonvisual element descriptors\n",
    "            # containing the type(s) of inspection text. We use  \"children's services inspection\"\n",
    "\n",
    "            nonvisual_text = pdf_link.select_one('span.nonvisual').text.lower().strip()\n",
    "\n",
    "            # For now at least, search terms hard-coded. \n",
    "            if 'children' in nonvisual_text and 'services' in nonvisual_text and 'inspection' in nonvisual_text:\n",
    "\n",
    "\n",
    "                # Create the filename and download the PDF (this filetype needs to be hard-coded here)\n",
    "                filename = nonvisual_text.replace(', pdf', '') + '.pdf'\n",
    "\n",
    "\n",
    "                pdf_content = requests.get(pdf_link['href']).content\n",
    "                with open(os.path.join(provider_dir, filename), 'wb') as f:\n",
    "                    f.write(pdf_content)\n",
    "\n",
    "\n",
    "               # Extract the local authority and inspection link, and add the data to the list\n",
    "                if not found_inspection_link:\n",
    "\n",
    "                    # Capture the data that will be exported about the most recent inspection only\n",
    "                    local_authority = provider_dir.split('_', 1)[-1].replace('_', ' ').strip()\n",
    "                    inspection_link = pdf_link['href']\n",
    "                    \n",
    "                    # Extract the report published date\n",
    "                    report_published_date_str = filename.split('-')[-1].strip().split('.')[0] # published date appears after '-' \n",
    "            \n",
    "                    # get/format date(s) (as dt objects)\n",
    "                    report_published_date = format_date(report_published_date_str, '%d %B %Y', '%d/%m/%y')\n",
    "\n",
    "                    # Now get the in-document data\n",
    "                    if pdf_data_capture:\n",
    "                        # Opt1 : ~x4 slower runtime\n",
    "                        # Only here if we have set PDF text scrape flag to True\n",
    "                        # Turn this off, speeds up script if we only need the inspection documents themselves to be retrieved\n",
    "\n",
    "                        # Scrape inside the pdf inspection reports\n",
    "                        inspection_data_dict = extract_inspection_data(pdf_content)\n",
    "                        \n",
    "\n",
    "                        # Dict extract here for readability of returned data/onward\n",
    "\n",
    "                        # inspection basics\n",
    "                        overall_effectiveness = inspection_data_dict['overall_inspection_grade']\n",
    "                        inspector_name = inspection_data_dict['inspector_name']\n",
    "                        inspection_start_date = inspection_data_dict['inspection_start_date']\n",
    "                        inspection_end_date = inspection_data_dict['inspection_end_date']\n",
    "                        inspection_framework = inspection_data_dict['inspection_framework']\n",
    "                        # additional inspection grades if available\n",
    "                        impact_of_leaders_grade = inspection_data_dict['impact_of_leaders_grade']\n",
    "                        help_and_protection_grade = inspection_data_dict['help_and_protection_grade']\n",
    "                        care_and_care_leavers_grade = inspection_data_dict['care_and_care_leavers_grade']\n",
    "                        # NLP extract \n",
    "                        sentiment_score = inspection_data_dict['sentiment_score']\n",
    "                        sentiment_summary = inspection_data_dict['sentiment_summary']\n",
    "                        main_inspection_topics = inspection_data_dict['main_inspection_topics']\n",
    "\n",
    "\n",
    "                        # format dates for output                       \n",
    "                        inspection_start_date_formatted = format_date_for_report(inspection_start_date, \"%d/%m/%Y\")\n",
    "                        inspection_end_date_formatted = format_date_for_report(inspection_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "                        # Format the provider directory as a file path link (in readiness for such as Excel)\n",
    "\n",
    "                        provider_dir_link = f\"{provider_dir}\"\n",
    "\n",
    "                        # depreciated - testing\n",
    "                        provider_dir_link = provider_dir_link.replace('/', '\\\\') # fix for Windows systems\n",
    "                        \n",
    "                        # file_link = f\"/{provider_dir}\"\n",
    "                        # provider_dir_link = 'file:///' + provider_dir.replace(\"\\\\\", \"/\") # to remove\n",
    "                                            \n",
    "                        data.append({\n",
    "                                        'urn': urn,\n",
    "                                        'local_authority': local_authority,\n",
    "                                        'inspection_link': inspection_link,\n",
    "                                        'overall_effectiveness_grade': overall_effectiveness,\n",
    "                                        'inspection_framework': inspection_framework,\n",
    "                                        'inspector_name': inspector_name,\n",
    "                                        'inspection_start_date': inspection_start_date_formatted,\n",
    "                                        'inspection_end_date': inspection_end_date_formatted,\n",
    "                                        'publication_date': report_published_date,\n",
    "                                        'local_link_to_all_inspections': provider_dir_link,\n",
    "                                        'impact_of_leaders_grade': impact_of_leaders_grade,\n",
    "                                        'help_and_protection_grade': help_and_protection_grade,\n",
    "                                        'care_and_care_leavers_grade': care_and_care_leavers_grade,\n",
    "                                        'sentiment_score': sentiment_score,\n",
    "                                        'sentiment_summary': sentiment_summary,\n",
    "                                        'main_inspection_topics': main_inspection_topics\n",
    "\n",
    "                                    })\n",
    "                        \n",
    "                    else:\n",
    "                        # Opt2 : ~x4 faster runtime\n",
    "                        # Only grab the data/docs we can get direct off the Ofsted page \n",
    "                        data.append({'urn': urn, 'local_authority': local_authority, 'inspection_link': inspection_link})\n",
    "\n",
    "                    \n",
    "                    found_inspection_link = True # Flag to ensure data reporting on only the most recent inspection\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_pagination(soup, url_stem):\n",
    "    \"\"\"\n",
    "    Handles pagination for a BeautifulSoup object representing a web page with paginated content.\n",
    "    \n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the web page.\n",
    "        url_stem (str): The base URL to which the relative path of the next page will be appended.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full URL of the next page if it exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the pagination element in the soup object\n",
    "    pagination = soup.find('ul', {'class': 'pagination'})\n",
    "\n",
    "    # Check if the pagination element exists\n",
    "    if pagination:\n",
    "        # Find the next page button in the pagination element\n",
    "        next_page_button = pagination.find('li', {'class': 'next'})\n",
    "\n",
    "        # Check if the next page button exists\n",
    "        if next_page_button:\n",
    "            # Extract the relative URL of the next page\n",
    "            next_page_url = next_page_button.find('a')['href']\n",
    "            \n",
    "            # Return the full URL of the next page by appending the relative URL to the base URL\n",
    "            return url_stem + next_page_url\n",
    "\n",
    "    # Return None if there is no next page button or pagination element\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_data(data, filename, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Exports data to a specified file type.\n",
    "\n",
    "    Args:\n",
    "        data (list or dict): The data to be exported.\n",
    "        filename (str): The desired name of the output file.\n",
    "        file_type (str, optional): The desired file type. Defaults to 'csv'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        filename_with_extension = filename + '.csv'\n",
    "        pd.DataFrame(data).to_csv(filename_with_extension, index=False)\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        filename_with_extension = filename + '.xlsx'\n",
    "        pd.DataFrame(data).to_excel(filename_with_extension, index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: unsupported file type '{file_type}'. Please choose 'csv' or 'xlsx'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{filename_with_extension} successfully created!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_data_update(data, filename, file_type='csv', hyperlink_column=None):\n",
    "    \"\"\"\n",
    "    Exports data to a specified file type.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): The data to be exported.\n",
    "        filename (str): The desired name of the output file.\n",
    "        file_type (str, optional): The desired file type. Defaults to 'csv'.\n",
    "        hyperlink_column (str, optional): The column containing folder names for hyperlinks. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        filename_with_extension = filename + '.csv'\n",
    "        data.to_csv(filename_with_extension, index=False)\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        filename_with_extension = filename + '.xlsx'\n",
    "\n",
    "        # Create a new workbook and add a worksheet\n",
    "        workbook = xlsxwriter.Workbook(filename_with_extension)\n",
    "        sheet = workbook.add_worksheet()\n",
    "\n",
    "        hyperlink_col_index = data.columns.get_loc(hyperlink_column) if hyperlink_column else None\n",
    "\n",
    "        # Define hyperlink format\n",
    "        hyperlink_format = workbook.add_format({'font_color': 'blue', 'underline': 1})\n",
    "\n",
    "\n",
    "        # Write DataFrame to the worksheet\n",
    "        for row_num, (index, row) in enumerate(data.iterrows(), start=1):\n",
    "            for col_num, (column, cell_value) in enumerate(row.items()):\n",
    "                if hyperlink_col_index is not None and col_num == hyperlink_col_index:\n",
    "                    # Add hyperlink using the HYPERLINK formula\n",
    "                    link = f\".\\\\{cell_value}\"\n",
    "                    sheet.write_formula(row_num, col_num, f'=HYPERLINK(\"{link}\", \"{cell_value}\")', hyperlink_format)\n",
    "\n",
    "                else:\n",
    "                    sheet.write(row_num, col_num, str(cell_value))\n",
    "\n",
    "        # Write header\n",
    "        header_format = workbook.add_format({'bold': True})\n",
    "        for col_num, column in enumerate(data.columns):\n",
    "            sheet.write(0, col_num, column, header_format)\n",
    "\n",
    "        # Save the workbook\n",
    "        workbook.close()\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: unsupported file type '{file_type}'. Please choose 'csv' or 'excel'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{filename_with_extension} successfully created!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_and_topics(pdf_buffer):\n",
    "    \"\"\"\n",
    "    Analyze the sentiment and extract the top 3 topics from a PDF document.\n",
    "\n",
    "    This function takes a file-like buffer containing a PDF document as input and\n",
    "    performs the following tasks:\n",
    "    1. Reads the content of the PDF file using the PyPDF2 library.\n",
    "    2. Extracts the text from each page and concatenates it into a single string.\n",
    "    3. Performs sentiment analysis on the extracted text using the TextBlob library.\n",
    "       The sentiment polarity score ranges from -1 (most negative) to 1 (most positive).\n",
    "    4. Identifies key themes or topics from the extracted text using the Latent Dirichlet\n",
    "       Allocation (LDA) model from the Gensim library.\n",
    "    5. Returns the sentiment polarity score and the top 3 topics extracted from the PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_buffer (io.BytesIO): A file-like buffer containing the PDF content.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sentiment polarity score (float) and a list of\n",
    "               the top 3 topics (strings).\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the PDF stuff\n",
    "    reader = PyPDF2.PdfReader(pdf_buffer)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Perform sentiment analysis on the extracted text\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    \n",
    "    # Identify key themes from the extracted text\n",
    "    # First, preprocess the text by tokenising and removing stop words\n",
    "    tokens = [word for sentence in blob.sentences for word in sentence.words]\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Create a dictionary from the tokenized text\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    \n",
    "    # Create a corpus from the dictionary and the tokenised text\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    \n",
    "    # Create an LDA model from the corpus\n",
    "    lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "    \n",
    "    # Get the top 3 topics from the LDA model\n",
    "    topics = [lda_model.print_topic(topic_num) for topic_num in range(3)]\n",
    "\n",
    "    # return sentiment, [extract_words(topic) for topic in topics] # Alternative to return the words with NO weights\n",
    "    return sentiment, topics\n",
    "\n",
    "\n",
    "# This an updated/extended version of the above \n",
    "def get_sentiment_and_sentiment_by_theme(pdf_buffer, theme1, theme2, theme3):\n",
    "    # Read the PDF stuff\n",
    "    reader = PyPDF2.PdfReader(pdf_buffer)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Perform sentiment analysis on the extracted text\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    \n",
    "    # Identify key themes from the extracted text\n",
    "    # First, preprocess the text by tokenising and removing stop words\n",
    "    tokens = [word for sentence in blob.sentences for word in sentence.words]\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Create a dictionary from the tokenized text\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    \n",
    "    # Create a corpus from the dictionary and the tokenised text\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "\n",
    "\n",
    "    # Create an LDA model from the corpus with a higher number of topics\n",
    "    lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "    \n",
    "    # Get all topics from the LDA model\n",
    "    all_topics = [lda_model.print_topic(topic_num) for topic_num in range(10)]\n",
    "\n",
    "    # Define a function to calculate similarity between two strings\n",
    "    def string_similarity(s1, s2):\n",
    "        vectorizer = CountVectorizer().fit_transform([s1, s2])\n",
    "        vectors = vectorizer.toarray()\n",
    "        return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "    # Filter topics based on the similarity to the provided theme strings\n",
    "    filtered_topics = []\n",
    "    themes = [theme1, theme2, theme3]\n",
    "    for topic in all_topics:\n",
    "        for theme in themes:\n",
    "            if string_similarity(topic, theme) > 0.2:  # Adjust the threshold as needed\n",
    "                filtered_topics.append(topic)\n",
    "                break\n",
    "\n",
    "    return sentiment, filtered_topics\n",
    "\n",
    "def get_sentiment_category(sentiment):\n",
    "    \"\"\"\n",
    "    Return the sentiment category based on the sentiment value.\n",
    "    For now, the ranges have been hard-coded. Might need to review that.\n",
    "\n",
    "    This function takes a sentiment value as input and returns the sentiment category\n",
    "    according to the following ranges:\n",
    "    - sentiment > 0.5: very positive\n",
    "    - 0 < sentiment <= 0.5: positive\n",
    "    - sentiment == 0: neutral\n",
    "    - -0.5 <= sentiment < 0: negative\n",
    "    - sentiment < -0.5: very negative\n",
    "\n",
    "    Args:\n",
    "        sentiment (float): Sentiment value ranging from -1 (most negative) to 1 (most positive).\n",
    "\n",
    "    Returns:\n",
    "        str: The sentiment category.\n",
    "    \"\"\"\n",
    "\n",
    "    if sentiment > 0.5:\n",
    "        return \"Sentiment very positive\"\n",
    "    elif 0 < sentiment <= 0.5:\n",
    "        return \"Sentiment positive\"\n",
    "    elif sentiment == 0:\n",
    "        return \"Sentiment neutral\"\n",
    "    elif -0.5 <= sentiment < 0:\n",
    "        return \"Sentiment negative\"\n",
    "    else:\n",
    "        return \"Sentiment very negative\"\n",
    "\n",
    "\n",
    "def extract_words(topic_string):\n",
    "    # Quick fix for when the sentiment weights per topic word not wanted.\n",
    "    words = re.findall(r'\\*\"(.*?)\"', topic_string)\n",
    "    return words\n",
    "\n",
    "\n",
    "def plot_filtered_topics(filtered_topics):\n",
    "    \"\"\"\n",
    "    Note: This only running if using func get_sentiment_and_sentiment_by_theme(pdf_buffer, theme1, theme2, theme3) \n",
    "\n",
    "    Visualise filtered inspection topics as a bar chart.\n",
    "\n",
    "    This function takes a list of filtered topics as input and creates a bar chart\n",
    "    to visualise the weighted words for each topic.\n",
    "\n",
    "    Args:\n",
    "        filtered_topics (list): List of filtered topics as strings.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Function to extract words and their weights from a topic string\n",
    "    def extract_words_weights(topic_string):\n",
    "        words_weights = [ww.split('*') for ww in topic_string.split(' + ')]\n",
    "        return [(float(weight.strip()), word.strip(\" '\\\"\")) for weight, word in words_weights]\n",
    "\n",
    "    # Extract words and their weights from the filtered_topics\n",
    "    topics_words_weights = [extract_words_weights(topic) for topic in filtered_topics]\n",
    "\n",
    "    # Create a bar chart for each topic\n",
    "    for idx, (words_weights, topic) in enumerate(zip(topics_words_weights, filtered_topics), 1):\n",
    "        words, weights = zip(*words_weights)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.barh(words, weights)\n",
    "        ax.set_xlabel('Weights')\n",
    "        ax.set_title(f'Topic {idx}: {topic[:50]}...')\n",
    "        ax.invert_yaxis()  # Invert y-axis to show higher weights at the top\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def import_csv_from_folder(folder_name):\n",
    "    \"\"\"\n",
    "    Imports a single CSV file from a local folder relative to the root of the script.\n",
    "\n",
    "    The CSV file must be located in the specified folder. If multiple CSV files are found,\n",
    "    a ValueError is raised. If no CSV files are found, a ValueError is raised.\n",
    "\n",
    "    Parameters:\n",
    "    folder_name (str): The name of the folder containing the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    file_names = [f for f in os.listdir(folder_name) if f.endswith('.csv')]\n",
    "    if len(file_names) == 0:\n",
    "        raise ValueError('No CSV file found in the specified folder')\n",
    "    elif len(file_names) > 1:\n",
    "        raise ValueError('More than one CSV file found in the specified folder')\n",
    "    else:\n",
    "        file_path = os.path.join(folder_name, file_names[0])\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "def merge_and_select_columns(df1, df2, key_column, columns_to_add):\n",
    "    \"\"\"\n",
    "    Merges two dataframes and returns a merged dataframe with additional columns from\n",
    "    the second dataframe, without any duplicate columns.\n",
    "\n",
    "    Parameters:\n",
    "    df1 (pandas.DataFrame): The first dataframe to merge.\n",
    "    df2 (pandas.DataFrame): The second dataframe to merge.\n",
    "    key_column (str): The name of the key column to merge on.\n",
    "    columns_to_add (list): A list of column names from df2 to add to df1.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new dataframe with merged data from df1 and selected columns from df2.\n",
    "    \"\"\"\n",
    "    merged = df1.merge(df2[columns_to_add + [key_column]], on=key_column)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def reorder_columns(df, key_col, columns_to_add):\n",
    "    \"\"\"\n",
    "    Reorders the columns of a dataframe to include the specified key column\n",
    "    and additional columns to add, without any duplicate or missing columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to reorder.\n",
    "    key_col (str): The name of the key column to include.\n",
    "    columns_to_add (list): A list of additional column names to include.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new dataframe with the reordered columns.\n",
    "    \"\"\"\n",
    "    # Create a list of columns to keep, removing duplicates and missing columns\n",
    "    columns_to_keep = list(dict.fromkeys(columns_to_add + df.columns.tolist()))\n",
    "\n",
    "    # Extract the index of the key column\n",
    "    key_col_index = df.columns.get_loc(key_col)\n",
    "\n",
    "    # Reorder the columns and insert the key column at its original position\n",
    "    reordered_columns = [key_col] + [c for c in columns_to_keep if c != key_col]\n",
    "    return df[reordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: May 09, 2023 3:24:33 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "May 09, 2023 3:24:34 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "\n",
      "Got stderr: May 09, 2023 3:24:57 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scrape data\n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    # Fetch and parse the HTML content of the current URL\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # Find all 'provider' links on the page\n",
    "    provider_links = soup.find_all('a', href=lambda href: href and '/provider/' in href)\n",
    "\n",
    "    # Process the provider links and extend the data list with the results\n",
    "    data.extend(process_provider_links(provider_links))\n",
    "\n",
    "    \n",
    "    # Since all results are on a single page, no need to handle pagination. \n",
    "    # Processing complete.   \n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'data' list to a DataFrame\n",
    "ilacs_inspection_summary_df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in additional flat-file stored data \n",
    "#\n",
    "\n",
    "# Enables broader potential onward usage/cross/backwards-compatible access \n",
    "# Here an example to enable easier re-use for future further enrichment, e.g. geospatial... \n",
    "# Note: Where possible, avoid any reliance on flat-file stored dynamic data! \n",
    "#       This process idealy only for static data, or where obtaining specific data points in a dynamic manner isnt possble etc. \n",
    "\n",
    "\n",
    "# Enrichment: LA codes\n",
    "# Ofsted data centres on URN, but some might need historic LA Number\n",
    "\n",
    "# import the needed external/local data\n",
    "local_authorities_lookup_df = import_csv_from_folder('import_data/la_lookup/') # bring data in \n",
    "\n",
    "# Ensure/Convert key col ('urn') consistency\n",
    "key_col = 'urn'\n",
    "ilacs_inspection_summary_df['urn'] = ilacs_inspection_summary_df['urn'].astype('int64')\n",
    "local_authorities_lookup_df['urn'] = local_authorities_lookup_df['urn'].astype('int64')\n",
    "\n",
    "# Define what data is required to be merged in\n",
    "columns_to_add = ['la_code', 'region_code']\n",
    "ilacs_inspection_summary_df = merge_and_select_columns(ilacs_inspection_summary_df, local_authorities_lookup_df, key_col, columns_to_add)\n",
    "\n",
    "# re-organise column structure now with new cols\n",
    "ilacs_inspection_summary_df = reorder_columns(ilacs_inspection_summary_df, key_col, columns_to_add)\n",
    "## End enrichment 1 ## \n",
    "\n",
    "\n",
    "\n",
    "# Enrichment: Anything else to enrich...? \n",
    "\n",
    "## End enrichment 2 ##\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofsted_childrens_services_overview.xlsx successfully created!\n"
     ]
    }
   ],
   "source": [
    "# Export summary data\n",
    "#\n",
    "\n",
    "\n",
    "# Also define the active hyperlink col if exporting to Excel\n",
    "save_data_update(ilacs_inspection_summary_df, export_summary_filename, file_type=export_file_type, hyperlink_column='local_link_to_all_inspections')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

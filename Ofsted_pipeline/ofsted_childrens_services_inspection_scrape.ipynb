{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ofsted Inspection Reports Scrape Tool \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary:</b><br>\n",
    "<span style=\"font-size:10pt\">Scrapes all local authorities 'Childrens Services Ofsted Inspection' reports in pdf format* and creates a summary overview report by LA with key inspection data points in either csv or xls format. The Children's Services full inspection reports(i.e. not interim nor monitoring visits) are downloaded locally as their original pdf, organised into folders** by local authority name and their URN. The LA naming is cleaned to better reflect a more standard/onward process use of LA naming - e.g. 'Nottingham City Council' becomes 'Nottingham' and 'Barnsley Metropolitan Borough Council' becomes 'Barnsley'. </span><br>\n",
    "    \n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>*from  from https://reports.ofsted.gov.uk/ .</li>\n",
    "    <li>**folder naming convention: \\provider_urn+local_authority_name(lowercase)...pdf inspection files.</li>\n",
    "</ul> \n",
    "\n",
    "\n",
    "<br>\n",
    "<b>N.B/Pre-requisites:</b><br>\n",
    "<span style=\"font-size:10pt\">Relies on Ofsted's continued use of nonvisual css element descriptors on the web site. Obv not ideal to rely on anything in the web-space, but any scrape process, however robust, is undermined/dictated by subsequent page changes. The tool has avoided the use of Selenium or similar as this is more likely to be impacted by visual design changes on the page(s). Instead it relies on the underlying php search process, and associated php generated links.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backlog/to-do:</b><br>\n",
    "\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>Moved to Trello: https://trello.com/c/4TihKpvQ</li>\n",
    "\n",
    "</ul> \n",
    "\n",
    "<b>Known bugs:</b><br>\n",
    "\n",
    "<ul style=\"font-size:10pt; list-style-type:disc; margin-left: 20px;\">\n",
    "    <li>Moved to Trello</li>\n",
    "    \n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Export options\n",
    "\n",
    "export_summary_filename = 'ofsted_childrens_services_overview'\n",
    "export_file_type         = 'csv' # Excel / csv currently supported\n",
    "\n",
    "\n",
    "# scrape inspection grade/data from pdf reports\n",
    "pdf_data_capture = True # True is default (scrape within pdf inspection reports)\n",
    "                        # This impacts run time E.g False == ~1m20 / True == ~ 4m10\n",
    "                        # False == only pdfs/list of LA's+link to most recent exported. Not grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ofsted site/page admin settings\n",
    "\n",
    "short_inspection_threshold    = 7 # ILACS inspection duration in days\n",
    "standard_inspection_threshold = 14\n",
    "\n",
    "max_page_results = 200 # Set max number of search results to show on page(MUST be > total number of LA's!) \n",
    "url_stem = 'https://reports.ofsted.gov.uk/'\n",
    "search_url = 'search?q=&location=&lat=&lon=&radius=&level_1_types=3&level_2_types%5B%5D=12' # On to-do list\n",
    "max_page_results_url = '&rows=' + str(max_page_results) # Coerce results page to display ALL providers on single results page without next/pagination\n",
    "\n",
    "# resultant complete url to process\n",
    "url = url_stem + search_url + max_page_results_url\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Script admin settings\n",
    "\n",
    "# Keep warnings quiet unless priority\n",
    "import logging\n",
    "import subprocess\n",
    "import warnings\n",
    "logging.getLogger('org.apache.pdfbox').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non-standard modules that might need installing\n",
    "# !pip install PyPDF2\n",
    "# !pip install tabula-py\n",
    "# !pip install textblob\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# pdf search/data extraction\n",
    "import io\n",
    "import tabula   \n",
    "import PyPDF2   \n",
    "import re       \n",
    "\n",
    "# used in handling inspection dates\n",
    "from dateutil import parser \n",
    "from datetime import datetime\n",
    "\n",
    "# nlp stuff for sentiment\n",
    "from textblob import TextBlob\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function defs\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Given a URL, returns a BeautifulSoup object.\n",
    "    Args: url (str): The URL to fetch and parse.\n",
    "    Returns: BeautifulSoup: The parsed HTML content.\n",
    "    \"\"\"\n",
    "    timeout_seconds = 10 # lets not assume the Ofsted page is up\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout_seconds)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except RequestException as e:\n",
    "        print(f\"An error occurred while fetching the URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_provider_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the la/provider name according to:\n",
    "                - expected output based on existing ILACS sheet\n",
    "                - historic string issues seen on Ofsted site\n",
    "\n",
    "    Args:\n",
    "        name (str): The original name to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned name.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    name = name.lower().replace('  ', ' ')\n",
    "    \n",
    "    # Remove specific phrases\n",
    "    name = name.replace(\"royal borough of \", \"\").replace(\"city of \", \"\").replace(\"metropolitan district council\", \"\").replace(\"london borough of\", \"\").replace(\"council of\", \"\")\n",
    "    \n",
    "    # Remove further undesired 'single' words and join the remaining parts\n",
    "    name_parts = [part for part in name.split() if part not in ['city', 'metropolitan', 'borough', 'council', 'county', 'district', 'the']]\n",
    "    return ' '.join(name_parts)\n",
    "\n",
    "\n",
    "def get_framework_type(start_date, end_date, short_inspection_threshold, standard_inspection_threshold):\n",
    "    \"\"\"\n",
    "    Returns an inspection framework type based on the duration between the start and end dates.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in the format \"dd/mm/yyyy\".\n",
    "        end_date (str): End date in the format \"dd/mm/yyyy\".\n",
    "\n",
    "    Returns:\n",
    "        str: Inspection framework type, which can be \"short\", \"standard\", or \"inspection duration longer than standard framework\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if both start and end dates have been accessible\n",
    "    if start_date is not None and end_date is not None:\n",
    "\n",
    "        # Check if end date is not earlier than start date\n",
    "        if end_date < start_date:\n",
    "            inspection_framework_str = \"invalid end or start date extracted\"\n",
    "\n",
    "        # Calculate the number of days between inspection start and end dates\n",
    "        else:\n",
    "            delta = end_date - start_date\n",
    "            inspection_duration_days = delta.days\n",
    "\n",
    "            # Determine the inspection framework based on the duration days\n",
    "            # Note: Needs further investigation to sense check real-world timeframes here, i.e. are thresholds 'working days'?\n",
    "            # For most instances this appears to be sufficiently accurate as-is. \n",
    "            if inspection_duration_days <= short_inspection_threshold:\n",
    "                inspection_framework_str = \"short\"\n",
    "            elif short_inspection_threshold < inspection_duration_days <= standard_inspection_threshold + 1:\n",
    "                inspection_framework_str = \"standard\"\n",
    "            else:\n",
    "                inspection_framework_str = \"inspection duration longer than standard framework\"\n",
    "\n",
    "    # Handle cases where start or end date is not provided \n",
    "    # Note: end date most likely to have not been extracted due to formatting issues\n",
    "    else:\n",
    "        inspection_framework_str = \"invalid date format\"\n",
    "\n",
    "    return inspection_framework_str\n",
    "\n",
    "\n",
    "def format_date(date_str: str, input_format: str, output_format: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert and format a date string.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): The input date string.\n",
    "        input_format (str): The format of the input date string.\n",
    "        output_format (str): The desired output format.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_str, input_format)\n",
    "    date_obj = dt.date()\n",
    "\n",
    "    return date_obj.strftime(output_format)\n",
    "\n",
    "\n",
    "def parse_date(date_str, input_format):\n",
    "    dt = datetime.strptime(date_str, input_format)\n",
    "\n",
    "    return dt.date()\n",
    "\n",
    "\n",
    "def format_date_for_report(date_obj, output_format_str):\n",
    "    \"\"\"\n",
    "    Formats a datetime object as a string in the d/m/y format, or returns an empty string if the input is None.\n",
    "\n",
    "    Args:\n",
    "        date_obj (datetime.datetime or None): The datetime object to format, or None.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted date string, or an empty string if date_obj is None.\n",
    "    \"\"\"\n",
    "    if date_obj is not None:\n",
    "        return date_obj.strftime(output_format_str)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_inspection_grade(row, column_name):\n",
    "    \"\"\"\n",
    "    Extracts the grade from the given row and column name. If the grade contains\n",
    "    the phrase \"requires improvement\", it returns the cleaned-up value.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from a Pandas DataFrame.\n",
    "        column_name (str): The name of the column containing the grade.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted grade.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the grade value cannot be converted to a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grade = str(row[column_name])\n",
    "\n",
    "        if \"requires improvement\" in grade.lower():\n",
    "            # Some RI text has further comment that we don't want, i.e. 'RI, *to become good*' \n",
    "            grade = \"Requires improvement\"\n",
    "        return grade\n",
    "    except Exception as e:\n",
    "        grade = f\"Unknown value type : {grade}\"\n",
    "        error_msg = f\"unknown value found: \\\"unknown : {grade}\\\"\"\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "\n",
    "\n",
    "def extract_inspection_data(pdf_content):\n",
    "    \"\"\"\n",
    "    Extracts the inspector's name, overall Ofsted grade, and inspection dates from the first page of a PDF report.\n",
    "\n",
    "    Args:\n",
    "        pdf_content (bytes): The content of the PDF file as bytes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the inspector's name, overall Ofsted grade, and inspection dates, or None if not found.\n",
    "\n",
    "    Notes:\n",
    "        This function extracts information from the first page of the PDF report. The inspector's name is extracted using a\n",
    "        regular expression search for the string \"Lead inspector:\". The overall Ofsted grade is extracted from a table that\n",
    "        appears on the first page of the report. The function uses the tabula library to extract the table data. The inspection\n",
    "        dates are also extracted using a regular expression search for the string \"Inspection dates:\". The function attempts to\n",
    "        parse the inspection dates into datetime objects and format them as \"dd/mm/yyyy\". The final output is a dictionary\n",
    "        containing the extracted information or None if any of the information could not be found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when an unknown grade value is found during grade extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a file-like buffer for the PDF content\n",
    "    with io.BytesIO(pdf_content) as buffer:\n",
    "        # Read the PDF content for text extraction\n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        page = reader.pages[0]\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Find the inspector's name using a regular expression\n",
    "        match = re.search(r\"Lead inspector:\\s*(.+)\", text)\n",
    "        if match:\n",
    "            inspector_name = match.group(1)\n",
    "            \n",
    "            inspector_name = inspector_name.split(',')[0].strip()       # Remove everything after the first comma (some contain '.., Her Majesty’s Inspector')\n",
    "            inspector_name = inspector_name.replace(\"HMI\", \"\").rstrip() # Remove \"HMI\" and any trailing spaces(some inspectors add this to name)\n",
    "\n",
    "        else:\n",
    "            inspector_name = None\n",
    "\n",
    "        # Read the PDF and extract the table on the first page\n",
    "        try:\n",
    "            buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "            tables = tabula.read_pdf(buffer, pages=1, multiple_tables=True)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the PDF: {e}\")\n",
    "            tables = []\n",
    "\n",
    "    # Initialize variables to store inspection grades\n",
    "    inspection_grade = None\n",
    "    impact_of_leaders_grade_str = None\n",
    "    help_and_protection_grade_str = None\n",
    "    care_and_care_leavers_grade_str = None\n",
    "\n",
    "    # Loop through tables to find the table containing inspection grades\n",
    "    # (Obv at the moment only 1, but just in case someone adds another)\n",
    "\n",
    "    # Loop through tables to find the table containing grades\n",
    "    for table in tables:\n",
    "        # Check if table contains necessary columns\n",
    "        if 'Judgement' in table.columns and 'Grade' in table.columns:\n",
    "            # Iterate through rows of the table\n",
    "            for index, row in table.iterrows():\n",
    "                # Convert judgement to lower case for case-insensitive matching\n",
    "                # Check if the value is NaN or Null and convert judgement to lower case for case-insensitive matching\n",
    "                if pd.isna(row['Judgement']):\n",
    "                    judgement = ''\n",
    "                else:\n",
    "                    judgement = str(row['Judgement']).lower()\n",
    "\n",
    "                # Check if report summary table/row contains 'Overall effectiveness'\n",
    "                if 'overall effectiveness' == judgement:\n",
    "                    inspection_grade = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The impact of leaders on social work practice with children and families'\n",
    "                elif re.search('impact of leaders', judgement):\n",
    "                    impact_of_leaders_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The experiences and progress of children who need help and protection'\n",
    "                elif re.search('help and protection', judgement):\n",
    "                    help_and_protection_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "                # Check if report summary table/row contains 'The experiences and progress of children in care and care leavers'\n",
    "                elif re.search('care and care leavers', judgement):\n",
    "                    care_and_care_leavers_grade_str = extract_inspection_grade(row, 'Grade')\n",
    "\n",
    "\n",
    "            # If inspection_grade is found and all other optional grades are found or not required, exit the loop\n",
    "            if inspection_grade is not None:\n",
    "                optional_grades = [impact_of_leaders_grade_str, help_and_protection_grade_str, care_and_care_leavers_grade_str]\n",
    "                if all(grade is not None for grade in optional_grades) or any(grade is None for grade in optional_grades):\n",
    "                    break\n",
    "\n",
    "\n",
    "    # Find the inspection dates using a regular expression\n",
    "    date_match = re.search(r\"Inspection dates:\\s*(.+)\", text)\n",
    "\n",
    "    if date_match:\n",
    "        # IF there was date data\n",
    "\n",
    "\n",
    "        inspection_dates = date_match.group(1).strip()\n",
    "            \n",
    "        # Some initial clean up based on historic data obs\n",
    "        inspection_dates = inspection_dates.replace(\".\", \"\")\n",
    "        inspection_dates = inspection_dates.replace(\"\\u00A0\", \" \") # Remove non-breaking space (Seen in nottingham report)\n",
    "        inspection_dates = re.sub(r\"[\\u2012\\u2013\\u2014\\u2212\\-]+\", \" to \", inspection_dates) # replace en dash char (\"\\u2013\"), em dash (\"\\u2014\"), or (\"-\") \n",
    "        inspection_dates = inspection_dates.split(\"and\")[0].strip() # Need this because we have such as :\n",
    "                                                                    # \"8 July 2019 to 12 July 2019 and 7 August 2019 to 8 August 2019\"\n",
    "                                                                    # E.g. Derbyshire\n",
    "        inspection_dates = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', inspection_dates) # Fix white spaces between date numbers e.g. \"wiltshire,\t1 9 June 2019\"\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(inspection_dates, str):\n",
    "            # data was as expected\n",
    "            year_match = re.search(r\"\\d{4}\", inspection_dates)\n",
    "            if year_match:\n",
    "                year = year_match.group(0) # get single copy of yyyy\n",
    "\n",
    "                # Now remove the year from the inspection_dates string\n",
    "                inspection_dates_cleaned = inspection_dates.replace(year, \"\").strip()\n",
    "\n",
    "            else:\n",
    "                # We had inspection_dates data but no recognisable year\n",
    "                year = None\n",
    "                inspection_dates_cleaned = inspection_dates\n",
    "\n",
    "        else:\n",
    "            # spurious data\n",
    "            # inspection_dates arrived with non-str, set default val\n",
    "            print(\"Error: inspection_dates is not a string. Type is\", type(inspection_dates))\n",
    "            inspection_dates_cleaned = None \n",
    "\n",
    "\n",
    "        # Now that we have already removed/cleaned those with 'and .....'\n",
    "        # Split the inspection_dates_cleaned string using ' to ' as the delimiter and limit the number of splits to 1\n",
    "        date_parts = inspection_dates_cleaned.split(' to ', maxsplit=1) # expect only 1 instance of 'to' between date vals\n",
    "        \n",
    "\n",
    "  \n",
    "        # Get the seperate inspection date(s) \n",
    "        start_date = date_parts[0].strip()\n",
    "        end_date = date_parts[1].strip() if len(date_parts) > 1 else None\n",
    "        \n",
    "        # Check if the month text is written in *both* the date strings\n",
    "        # Required work-around as Ofsted reports contain inspection date strings in multiple formats (i/ii/iii...)\n",
    "        #   i)      \"15 to 26 November\"  \n",
    "        #   ii)     \"28 February to 4 March\" or \"8 October to 19 October\" (majority)\n",
    "        #   iii)    ['8 July ', '12 July   and 7 August  to'] (*recently seen)\n",
    "        #   iv)     \"11 September 2017 to 5 October 2017\" (double year)\n",
    "        #   v)      \"Inspection dates: 19 November–30 November 2018\" (Bromley)\n",
    "        if len(start_date) <= 2: # i.e. do we only have a date with no month text\n",
    "            inspection_month = end_date.split()[1]\n",
    "            start_date = f\"{start_date} {inspection_month}\"\n",
    "\n",
    "        # Append the inspection year to the start_date and end_date\n",
    "        start_date_str = f\"{start_date} {year}\"\n",
    "        end_date_str = f\"{end_date} {year}\" if end_date else None\n",
    "\n",
    "\n",
    "        # format current str dates (as dt objects)\n",
    "        start_date_formatted = parse_date(start_date_str, '%d %B %Y') #  str '8 January 2021' \n",
    "        end_date_formatted = parse_date(end_date_str, '%d %B %Y')\n",
    "\n",
    "        # calculate inspection duration and return framework string\n",
    "        # Note: Problems arising here generally relate to the end_date extraction from pdf\n",
    "        inspection_framework_str = get_framework_type(start_date_formatted, end_date_formatted, short_inspection_threshold, standard_inspection_threshold)\n",
    "\n",
    "    else:\n",
    "        # unable to extract the data or didnt exist\n",
    "        start_date_formatted = None\n",
    "        end_date_formatted = None\n",
    "        inspection_framework_str = None\n",
    "\n",
    "\n",
    "    return {'inspector_name': inspector_name, \n",
    "            'overall_inspection_grade': inspection_grade,\n",
    "            'inspection_start_date': start_date_formatted,\n",
    "            'inspection_end_date': end_date_formatted,\n",
    "            'inspection_framework': inspection_framework_str,\n",
    "            'impact_of_leaders_grade': impact_of_leaders_grade_str,\n",
    "            'help_and_protection_grade': help_and_protection_grade_str,\n",
    "            'care_and_care_leavers_grade': care_and_care_leavers_grade_str\n",
    "            }\n",
    "\n",
    "\n",
    "def process_provider_links(provider_links):\n",
    "    \"\"\"\n",
    "    Processes provider links and returns a list of dictionaries containing URN, local authority, and inspection link.\n",
    "\n",
    "    Args:\n",
    "        provider_links (list): A list of BeautifulSoup Tag objects representing provider links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing URN, local authority, inspection link, and, if enabled, additional inspection data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    global pdf_data_capture # Bool flag\n",
    "    \n",
    "    for link in provider_links:\n",
    "        # Extract the URN and provider name from the web link shown\n",
    "        urn = link['href'].rsplit('/', 1)[-1]\n",
    "        name = clean_provider_name(link.text.strip())\n",
    "\n",
    "        # Create the provider directory path\n",
    "        provider_dir = os.path.join('.', urn + '_' + name)\n",
    "\n",
    "        # Create the provider directory if it doesn't exist\n",
    "        if not os.path.exists(provider_dir):\n",
    "            os.makedirs(provider_dir)\n",
    "\n",
    "        # Get the child page content\n",
    "        child_url = 'https://reports.ofsted.gov.uk' + link['href']\n",
    "        child_soup = get_soup(child_url)\n",
    "\n",
    "        # Find all publication links in the provider's child page\n",
    "        pdf_links = child_soup.find_all('a', {'class': 'publication-link'})\n",
    "\n",
    "        # Initialize a flag to indicate if an inspection link has been found\n",
    "        # Important: This assumes that the provider's reports are returned/organised most recent FIRST\n",
    "        found_inspection_link = False\n",
    "\n",
    "        # Iterate through the publication links\n",
    "        for pdf_link in pdf_links:\n",
    "\n",
    "            # Check if the current/next href-link meets the selection criteria\n",
    "            # This block obv relies on Ofsted continued use of nonvisual element descriptors\n",
    "            # containing the type(s) of inspection text. We use  \"children's services inspection\"\n",
    "\n",
    "            nonvisual_text = pdf_link.select_one('span.nonvisual').text.lower().strip()\n",
    "\n",
    "            # For now at least, search terms hard-coded. \n",
    "            if 'children' in nonvisual_text and 'services' in nonvisual_text and 'inspection' in nonvisual_text:\n",
    "\n",
    "\n",
    "                # Create the filename and download the PDF (this filetype needs to be hard-coded here)\n",
    "                filename = nonvisual_text.replace(', pdf', '') + '.pdf'\n",
    "\n",
    "                pdf_content = requests.get(pdf_link['href']).content\n",
    "                with open(os.path.join(provider_dir, filename), 'wb') as f:\n",
    "                    f.write(pdf_content)\n",
    "\n",
    "\n",
    "               # Extract the local authority and inspection link, and add the data to the list\n",
    "                if not found_inspection_link:\n",
    "\n",
    "                    # Capture the data that will be exported about the most recent inspection only\n",
    "                    local_authority = provider_dir.split('_', 1)[-1].replace('_', ' ').strip()\n",
    "                    inspection_link = pdf_link['href']\n",
    "                    \n",
    "                    # Extract the report published date\n",
    "                    report_published_date_str = filename.split('-')[-1].strip().split('.')[0] # published date appears after '-' \n",
    "            \n",
    "                    # get/format date(s) (as dt objects)\n",
    "                    report_published_date = format_date(report_published_date_str, '%d %B %Y', '%d/%m/%y')\n",
    "\n",
    "                    # Now get the in-document data\n",
    "                    if pdf_data_capture:\n",
    "                        # Opt1 : ~x4 slower runtime\n",
    "                        # Only here if we have set PDF text scrape flag to True\n",
    "                        # Turn this off, speeds up script if we only need the inspection documents themselves to be retrieved\n",
    "\n",
    "                        # Scrape inside the pdf inspection reports\n",
    "                        inspection_data_dict = extract_inspection_data(pdf_content)\n",
    "                        \n",
    "                        # Added for readability of returned data/onward\n",
    "                        overall_effectiveness = inspection_data_dict['overall_inspection_grade']\n",
    "                        inspector_name = inspection_data_dict['inspector_name']\n",
    "                        inspection_start_date = inspection_data_dict['inspection_start_date']\n",
    "                        inspection_end_date = inspection_data_dict['inspection_end_date']\n",
    "                        inspection_framework = inspection_data_dict['inspection_framework']\n",
    "\n",
    "                        impact_of_leaders_grade = inspection_data_dict['impact_of_leaders_grade']\n",
    "                        help_and_protection_grade = inspection_data_dict['help_and_protection_grade']\n",
    "                        care_and_care_leavers_grade = inspection_data_dict['care_and_care_leavers_grade']\n",
    "\n",
    "                        # format dates for output                       \n",
    "                        inspection_start_date_formatted = format_date_for_report(inspection_start_date, \"%d/%m/%Y\")\n",
    "                        inspection_end_date_formatted = format_date_for_report(inspection_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "                        # Format the provider directory as a file path link for Excel\n",
    "                        # needs a fix\n",
    "                        provider_dir_link = 'file:///' + provider_dir.replace(\"\\\\\", \"/\")\n",
    "                                            \n",
    "                        data.append({\n",
    "                                        'urn': urn,\n",
    "                                        'local_authority': local_authority,\n",
    "                                        'inspection_link': inspection_link,\n",
    "                                        'overall_effectiveness_grade': overall_effectiveness,\n",
    "                                        'inspection_framework': inspection_framework,\n",
    "                                        'inspector_name': inspector_name,\n",
    "                                        'inspection_start_date': inspection_start_date_formatted,\n",
    "                                        'inspection_end_date': inspection_end_date_formatted,\n",
    "                                        'publication_date': report_published_date,\n",
    "                                        'local_link_to_all_inspections': provider_dir_link,\n",
    "                                        'impact_of_leaders_grade': impact_of_leaders_grade,\n",
    "                                        'help_and_protection_grade': help_and_protection_grade,\n",
    "                                        'care_and_care_leavers_grade': care_and_care_leavers_grade\n",
    "\n",
    "                                    })\n",
    "                        \n",
    "                    else:\n",
    "                        # Opt2 : ~x4 faster runtime\n",
    "                        # Only grab the data/docs we can get direct off the Ofsted page \n",
    "                        data.append({'urn': urn, 'local_authority': local_authority, 'inspection_link': inspection_link})\n",
    "\n",
    "                    \n",
    "                    found_inspection_link = True # Flag to ensure data reporting on only the most recent inspection\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_pagination(soup, url_stem):\n",
    "    \"\"\"\n",
    "    Handles pagination for a BeautifulSoup object representing a web page with paginated content.\n",
    "    \n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the web page.\n",
    "        url_stem (str): The base URL to which the relative path of the next page will be appended.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full URL of the next page if it exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the pagination element in the soup object\n",
    "    pagination = soup.find('ul', {'class': 'pagination'})\n",
    "\n",
    "    # Check if the pagination element exists\n",
    "    if pagination:\n",
    "        # Find the next page button in the pagination element\n",
    "        next_page_button = pagination.find('li', {'class': 'next'})\n",
    "\n",
    "        # Check if the next page button exists\n",
    "        if next_page_button:\n",
    "            # Extract the relative URL of the next page\n",
    "            next_page_url = next_page_button.find('a')['href']\n",
    "            \n",
    "            # Return the full URL of the next page by appending the relative URL to the base URL\n",
    "            return url_stem + next_page_url\n",
    "\n",
    "    # Return None if there is no next page button or pagination element\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_data(data, filename, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Exports data to a specified file type.\n",
    "\n",
    "    Args:\n",
    "        data (list or dict): The data to be exported.\n",
    "        filename (str): The desired name of the output file.\n",
    "        file_type (str, optional): The desired file type. Defaults to 'csv'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        filename_with_extension = filename + '.csv'\n",
    "        pd.DataFrame(data).to_csv(filename_with_extension, index=False)\n",
    "\n",
    "    elif file_type == 'excel':\n",
    "        filename_with_extension = filename + '.xlsx'\n",
    "        pd.DataFrame(data).to_excel(filename_with_extension, index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: unsupported file type '{file_type}'. Please choose 'csv' or 'xlsx'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{filename_with_extension} successfully created!\")\n",
    "\n",
    "\n",
    "def get_sentiment_and_topics(pdf_filename):\n",
    "    # Read the PDF file\n",
    "    with open(pdf_filename, 'rb') as buffer:\n",
    "        reader = PyPDF2.PdfReader(buffer)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Perform sentiment analysis on the extracted text\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    \n",
    "    # Identify key themes from the extracted text\n",
    "    # First, preprocess the text by tokenizing and removing stop words\n",
    "    tokens = [word for sentence in blob.sentences for word in sentence.words]\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Create a dictionary from the tokenized text\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    \n",
    "    # Create a corpus from the dictionary and the tokenized text\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    \n",
    "    # Create an LDA model from the corpus\n",
    "    lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "    \n",
    "    # Get the top 3 topics from the LDA model\n",
    "    topics = [lda_model.print_topic(topic_num) for topic_num in range(3)]\n",
    "    \n",
    "    return sentiment, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NE', '393']\n"
     ]
    }
   ],
   "source": [
    "# # In Progress / New Dev\n",
    "# # LA to region lookup. For now, this hard-coded. \n",
    "\n",
    "local_authorities_lookup = {\"Brent\": [\"GL\", \"304\"], \"Birmingham\": [\"WM\", \"330\"], \"Haringey\": [\"GL\", \"309\"], \"Bexley\": [\"GL\", \"303\"], \"East Riding of Yorkshire\": [\"YH\", \"811\"], \"Hertfordshire\": [\"E\", \"919\"], \"Rochdale\": [\"NW\", \"354\"], \"Slough\": [\"SE\", \"871\"], \"Bristol\": [\"SW\", \"801\"], \"Blackpool\": [\"NW\", \"890\"], \"Lancashire\": [\"NW\", \"888\"], \"Trafford\": [\"NW\", \"358\"], \"Bradford\": [\"YH\", \"380\"], \"Kingston upon Hull\": [\"YH\", \"810\"], \"Norfolk\": [\"E\", \"926\"], \"Wandsworth\": [\"GL\", \"212\"], \"Solihull\": [\"WM\", \"334\"], \"Dudley\": [\"WM\", \"332\"], \"Lambeth\": [\"GL\", \"208\"], \"North Lincolnshire\": [\"YH\", \"813\"], \"Darlington\": [\"NE\", \"841\"], \"Stoke-on-Trent\": [\"WM\", \"861\"], \"West Northamptonshire\": [\"EM\", \"928\"], \"North Northamptonshire\": [\"EM\", \"928\"], \"Southwark\": [\"GL\", \"210\"], \"Cumbria\": [\"NW\", \"909\"], \"Herefordshire\": [\"WM\", \"884\"], \"Newham\": [\"GL\", \"316\"], \"Somerset\": [\"SW\", \"933\"], \"Nottingham\": [\"EM\", \"892\"], \"Luton\": [\"E\", \"821\"], \"Rotherham\": [\"YH\", \"372\"], \"Coventry\": [\"WM\", \"331\"], \"Redcar and Cleveland\": [\"NE\", \"807\"], \"Bracknell Forest\": [\"SE\", \"867\"], \"Durham\": [\"NE\", \"840\"], \"Kent\": [\"SE\", \"886\"], \"Sandwell\": [\"WM\", \"333\"], \"Wigan\": [\"NW\", \"359\"], \"Camden\": [\"GL\", \"202\"], \"Stockport\": [\"NW\", \"356\"], \"Wolverhampton\": [\"WM\", \"336\"], \"Manchester\": [\"NW\", \"352\"], \"Derby\": [\"EM\", \"831\"], \"Torbay\": [\"SW\", \"880\"], \"West Berkshire\": [\"SE\", \"869\"], \"York\": [\"YH\", \"816\"], \"Bath and North East Somerset\": [\"SW\", \"800\"], \"Merton\": [\"GL\", \"315\"], \"Leeds\": [\"YH\", \"383\"], \"Sefton\": [\"NW\", \"343\"], \"Doncaster\": [\"YH\", \"371\"], \"Gloucestershire\": [\"SW\", \"916\"], \"Shropshire\": [\"WM\", \"893\"], \"Richmond upon Thames\": [\"GL\", \"318\"], \"Blackburn with Darwen\": [\"NW\", \"889\"], \"Central Bedfordshire\": [\"E\", \"823\"], \"Surrey\": [\"SE\", \"936\"], \"Sutton\": [\"GL\", \"319\"], \"Bournemouth, Christchurch & Poole\": [\"SW\", \"839\"], \"Buckinghamshire\": [\"SE\", \"825\"], \"Newcastle upon Tyne\": [\"NE\", \"391\"], \"Warwickshire\": [\"WM\", \"937\"], \"Bedford\": [\"E\", \"822\"], \"Wakefield\": [\"YH\", \"384\"], \"Bury\": [\"NW\", \"351\"], \"Milton Keynes\": [\"SE\", \"826\"], \"Knowsley\": [\"NW\", \"340\"], \"North East Lincolnshire\": [\"YH\", \"812\"], \"Walsall\": [\"WM\", \"335\"], \"Dorset\": [\"SW\", \"835\"], \"Leicester\": [\"EM\", \"856\"], \"Sunderland\": [\"NE\", \"394\"], \"Islington\": [\"GL\", \"206\"], \"North Tyneside\": [\"NE\", \"392\"], \"City of London\": [\"GL\", \"201\"], \"Halton\": [\"NW\", \"876\"], \"Rutland\": [\"EM\", \"857\"], \"North Somerset\": [\"SW\", \"802\"], \"Harrow\": [\"GL\", \"310\"], \"Croydon\": [\"GL\", \"306\"], \"Northumberland\": [\"NE\", \"929\"], \"Telford and Wrekin\": [\"WM\", \"894\"], \"Devon\": [\"SE\", \"878\"], \"Windsor and Maidenhead\": [\"SE\", \"868\"], \"Greenwich\": [\"GL\", \"203\"], \"Middlesbrough\": [\"NE\", \"806\"], \"Cheshire East\": [\"NW\", \"895\"], \"Southampton\": [\"SE\", \"852\"], \"Hackney\": [\"GL\", \"204\"], \"Thurrock\": [\"E\", \"883\"], \"Ealing\": [\"GL\", \"307\"], \"Kingston upon Thames\": [\"GL\", \"314\"], \"Cornwall\": [\"SW\", \"908\"], \"Isles of Scilly\": [\"SW\", \"420\"], \"Nottinghamshire\": [\"EM\", \"891\"], \"Leicestershire\": [\"EM\", \"855\"], \"St Helens\": [\"NW\", \"342\"], \"Reading\": [\"SE\", \"870\"], \"Hammersmith and Fulham\": [\"GL\", \"205\"], \"Kensington and Chelsea\": [\"GL\", \"207\"], \"Westminster\": [\"GL\", \"213\"], \"Medway\": [\"SE\", \"887\"], \"Sheffield\": [\"YH\", \"373\"], \"Southend-on-Sea\": [\"E\", \"882\"], \"Derbyshire\": [\"EM\", \"830\"], \"Lewisham\": [\"GL\", \"209\"], \"Warrington\": [\"NW\", \"877\"], \"Swindon\": [\"SW\", \"866\"], \"Northamptonshire\": [\"EM\", \"928\"], \"Wirral\": [\"NW\", \"344\"], \"Worcestershire\": [\"WM\", \"885\"], \"Kirklees\": [\"YH\", \"382\"], \"Tower Hamlets\": [\"GL\", \"211\"], \"Wiltshire\": [\"SW\", \"865\"], \"Wokingham\": [\"SE\", \"872\"], \"Stockton-on-Tees\": [\"NE\", \"808\"], \"Barnet\": [\"GL\", \"302\"], \"Tameside\": [\"NW\", \"357\"], \"Gateshead\": [\"NE\", \"390\"], \"Hampshire\": [\"SE\", \"850\"], \"Lincolnshire\": [\"EM\", \"925\"], \"Redbridge\": [\"GL\", \"317\"], \"Suffolk\": [\"E\", \"935\"], \"Cheshire West and Chester\": [\"NW\", \"896\"], \"Enfield\": [\"GL\", \"308\"], \"South Gloucestershire\": [\"SW\", \"803\"], \"West Sussex\": [\"SE\", \"938\"], \"Barking and Dagenham\": [\"GL\", \"301\"], \"Staffordshire\": [\"WM\", \"860\"], \"Waltham Forest\": [\"GL\", \"320\"], \"Oldham\": [\"NW\", \"353\"], \"Cambridgeshire\": [\"E\", \"873\"], \"Bromley\": [\"GL\", \"305\"], \"Essex\": [\"E\", \"881\"], \"Isle of Wight\": [\"SE\", \"921\"], \"Calderdale\": [\"YH\", \"381\"], \"Plymouth\": [\"SW\", \"879\"], \"Barnsley\": [\"YH\", \"370\"], \"Salford\": [\"NW\", \"355\"], \"Hounslow\": [\"GL\", \"313\"], \"Portsmouth\": [\"SE\", \"851\"], \"Hartlepool\": [\"NE\", \"805\"], \"Bournemouth\": [\"SW\", \"837\"], \"East Sussex\": [\"SE\", \"845\"], \"Brighton and Hove\": [\"SE\", \"846\"], \"North Yorkshire\": [\"YH\", \"815\"], \"Peterborough\": [\"E\", \"874\"], \"Havering\": [\"GL\", \"311\"], \"Liverpool\": [\"NW\", \"341\"], \"Bolton\": [\"NW\", \"350\"], \"Hillingdon\": [\"GL\", \"312\"], \"Oxfordshire\": [\"SE\", \"931\"], \"South Tyneside\": [\"NE\", \"393\"] }\n",
    "# test\n",
    "print(local_authorities_lookup[\"South Tyneside\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: May 02, 2023 3:07:13 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "May 02, 2023 3:07:14 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n",
      "\n",
      "Got stderr: May 02, 2023 3:07:28 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n",
      "Got stderr: May 02, 2023 3:08:00 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofsted_childrens_services_overview.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scrape data\n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    # Fetch and parse the HTML content of the current URL\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # Find all 'provider' links on the page\n",
    "    provider_links = soup.find_all('a', href=lambda href: href and '/provider/' in href)\n",
    "\n",
    "    # Process the provider links and extend the data list with the results\n",
    "    data.extend(process_provider_links(provider_links))\n",
    "\n",
    "    \n",
    "    # Since all results are on a single page, no need to handle pagination. \n",
    "    # Processing complete.   \n",
    "    break\n",
    "\n",
    "\n",
    "# Export summary data\n",
    "save_data(data, export_summary_filename, export_file_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
